{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3f8fda-7b0a-4db0-93dc-f8ba8350508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Version: 2.1.3\n",
      "Pandas Version: 2.2.3\n",
      "Seaborn Version: 0.13.2\n",
      "Matplotlib Version: 3.9.2\n",
      "Python Version: 3.11.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# custom scoring\n",
    "from sklearn import metrics\n",
    "# split test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# seed\n",
    "RANDOM_STATE = 1776\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# print versions\n",
    "print(\"Numpy Version: \" + np.__version__)\n",
    "print(\"Pandas Version: \" + pd.__version__)\n",
    "print(\"Seaborn Version: \" + sns.__version__)\n",
    "print(\"Matplotlib Version: \" + plt.matplotlib.__version__)\n",
    "print(\"Python Version: \" + python_version())\n",
    "\n",
    "# adjust pandas display options to max\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# adjust pandas display options to ensure full display of content\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5797574d-b8b1-41aa-9b50-5e2e23513f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full dataframe\n",
    "df = pd.read_pickle(\"../Data/CleanFullLabelsML.pkl\")\n",
    "\n",
    "# data dictionary\n",
    "df_dict = pd.read_pickle(\"../Data/FinalcolumnDefinitionML.pkl\")\n",
    "\n",
    "# data label\n",
    "df_label = pd.read_pickle(\"../Data/colLabelML.pkl\") \n",
    "\n",
    "# boolean\n",
    "df_bool = pd.read_pickle(\"../Data/colBooleanML.pkl\")\n",
    "\n",
    "# nominal\n",
    "df_nominal = pd.read_pickle(\"../Data/colNominalML.pkl\")\n",
    "\n",
    "# ordinal\n",
    "df_ordinal = pd.read_pickle(\"../Data/colOrdinalML.pkl\")\n",
    "\n",
    "# numeric\n",
    "df_numeric = pd.read_pickle(\"../Data/colNumericML.pkl\")\n",
    "\n",
    "# checking for duplicated column name\n",
    "df.columns[df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833106ac-777d-4006-b928-f47cfb1ce366",
   "metadata": {},
   "source": [
    "### User function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c331907-857c-463a-bf03-106237d8e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeColumn(data, col):\n",
    "    \"\"\"\n",
    "    Remove unwanted columns\n",
    "    \"\"\"\n",
    "    # display removed feature(s)\n",
    "    print(f\"\\nRemoved Features:{col}\\n\")\n",
    "    # display shape of DataFrame\n",
    "    print(f\"Total rows before: {data.shape[0]:,} & columns: {data.shape[1]:,}\")\n",
    "    \n",
    "    # remove column\n",
    "    data.drop(columns=col, axis=1, inplace=True)\n",
    "\n",
    "    # reset index in place\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # display shape of DataFrame\n",
    "    print(f\"Total rows after: {data.shape[0]:,} & columns: {data.shape[1]:,}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def removeRowUsingMask(data, removeColLst, colstr):\n",
    "    # boolean mask\n",
    "    mask = ~data[colstr].isin(removeColLst)\n",
    "    \n",
    "    # apply the mask to keep only rows where 'removeColLst'\n",
    "    data = data[mask]\n",
    "    \n",
    "    # reset the index if needed\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # disply row removed msg\n",
    "    print(f\"Remove row(s) from df_{colstr} DataFrame.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def removeHouseKeeping(data, removeColLst, dataBool, dataOrdinal, dataNominal, dataNumeric, dataLabel):\n",
    "    \"\"\"\n",
    "    Run helper fuction for house keeping\n",
    "    \"\"\"\n",
    "    # remove DataFrame data (house keeping)\n",
    "    dataLabel = removeRowUsingMask(dataLabel, removeColLst, colstr='label')\n",
    "    dataBool = removeRowUsingMask(dataBool, removeColLst, colstr='boolean')\n",
    "    dataOrdinal = removeRowUsingMask(dataOrdinal, removeColLst, colstr='ordinal')\n",
    "    dataNominal = removeRowUsingMask(dataNominal, removeColLst, colstr='nominal')\n",
    "    dataNumeric = removeRowUsingMask(dataNumeric, removeColLst, colstr='numeric')\n",
    "    \n",
    "    # remove features\n",
    "    data = removeColumn(data, removeColLst)\n",
    "\n",
    "    return data, dataBool, dataOrdinal, dataNominal, dataNumeric, dataLabel\n",
    "\n",
    "\n",
    "def datatypeDF(data, databool, datanominal, dataordinal, datanumeric):    \n",
    "    # initialize variables for all the column name per each datatype\n",
    "    boolCol = databool.boolean.to_list()\n",
    "    nominalCol = datanominal.nominal.to_list()\n",
    "    ordinalCol = dataordinal.ordinal.to_list()\n",
    "    numericCol = datanumeric.numeric.to_list()\n",
    "\n",
    "    print('Total Data feature count: ', df.shape[1])\n",
    "    print(f\"\\nBoolean feature count: {len(boolCol)}\")\n",
    "    print(f\"Nominal feature count: {len(nominalCol)}\")\n",
    "    print(f\"Ordinal feature count: {len(ordinalCol)}\")\n",
    "    print(f\"Numeric feature count: {len(numericCol)}\")\n",
    "    print('\\nTotal feature count: ' ,len(boolCol) + len(nominalCol) + len(ordinalCol) + len(numericCol))\n",
    "\n",
    "    # return list for each type\n",
    "    return boolCol, nominalCol, ordinalCol, numericCol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b82ba18-5317-48ec-b9ad-f91bb9bce7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove row(s) from df_label DataFrame.\n",
      "Remove row(s) from df_boolean DataFrame.\n",
      "Remove row(s) from df_ordinal DataFrame.\n",
      "Remove row(s) from df_nominal DataFrame.\n",
      "Remove row(s) from df_numeric DataFrame.\n",
      "\n",
      "Removed Features:['FollowUpFunctionalStatus_CAN', 'AirwayDehiscencePostTransplant_CAN', 'AcuteRejectionEpisode_CAN', 'StrokePostTransplant_CAN', 'PacemakerPostTransplant_CAN', 'GraftFailed_CAN', 'LastFollowupNumber_CAN', 'RecipientStatus_CAN', 'RejectionTreatmentWithinOneYear_CAN', 'GraftStatus_CAN', 'LengthOfStay_CAN']\n",
      "\n",
      "Total rows before: 14,856 & columns: 121\n",
      "Total rows after: 14,856 & columns: 110\n"
     ]
    }
   ],
   "source": [
    "# select label for classification\n",
    "removeCol = df_label.label.to_list()\n",
    "\n",
    "# remove GraftFailed_CAN\n",
    "removeCol.remove('TransplantStatus_CAN')\n",
    "removeCol.remove('TransplantSurvivalDay_CAN')\n",
    "\n",
    "# remove unwanted features\n",
    "df, df_bool, df_ordinal, df_nominal, df_numeric, df_label = removeHouseKeeping(df, removeCol, df_bool, df_ordinal, df_nominal, df_numeric, df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b1b559-027a-4514-bebb-52eddcbb433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data feature count:  110\n",
      "\n",
      "Boolean feature count: 9\n",
      "Nominal feature count: 69\n",
      "Ordinal feature count: 15\n",
      "Numeric feature count: 17\n",
      "\n",
      "Total feature count:  110\n"
     ]
    }
   ],
   "source": [
    "# initialize list with feature names\n",
    "boolCol, nominalCol, ordinalCol, numericCol = datatypeDF(df, df_bool, df_nominal, df_ordinal, df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f55a54-cccd-4e31-b010-8cc53fd9a713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TransplantStatus_CAN</th>\n",
       "      <td>14856</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>13005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransplantSurvivalDay_CAN</th>\n",
       "      <td>14856.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>634.23371</td>\n",
       "      <td>473.942355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>1084.0</td>\n",
       "      <td>1799.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             count unique    top   freq       mean  \\\n",
       "TransplantStatus_CAN         14856      2  False  13005        NaN   \n",
       "TransplantSurvivalDay_CAN  14856.0    NaN    NaN    NaN  634.23371   \n",
       "\n",
       "                                  std  min    25%    50%     75%     max  \n",
       "TransplantStatus_CAN              NaN  NaN    NaN    NaN     NaN     NaN  \n",
       "TransplantSurvivalDay_CAN  473.942355  0.0  194.0  606.0  1084.0  1799.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe\n",
    "df[['TransplantStatus_CAN','TransplantSurvivalDay_CAN']].describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456f934d-e9c1-4200-9d1d-5dda9065f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df = df.rename(columns={'TransplantStatus_CAN': 'status', 'TransplantSurvivalDay_CAN': 'time'})\n",
    "\n",
    "# create a structured array for survival data\n",
    "y = np.zeros(len(df), dtype=[('status', bool), ('time', float)])\n",
    "y['status'] = df['status'].astype(bool)\n",
    "y['time'] = df['time'].astype(float)\n",
    "X = df.drop(columns=['status', 'time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e8073-3067-4a51-a367-601aced84541",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9010d0-456f-437a-aaab-acdf48452a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from list\n",
    "boolCol.remove('TransplantStatus_CAN')\n",
    "numericCol.remove('TransplantSurvivalDay_CAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7c636-9b15-4d31-b93f-ab9929dad5aa",
   "metadata": {},
   "source": [
    "### Split Testing & Validation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb821cae-ba1e-493f-ad2c-f54d54d864ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncodeDummyScaleTrainValTest(Xdata, ydata, nominalColumns, numericColumns, seed=RANDOM_STATE):\n",
    "\n",
    "    # dummy Encoding\n",
    "    df_encoded = pd.get_dummies(Xdata, columns=nominalColumns, drop_first=True)\n",
    "    \n",
    "    # split the dataset into 80% training and 20% testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_encoded, ydata, test_size=0.2, random_state=seed, stratify=y['status'])\n",
    "    \n",
    "    # split train data into validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed, stratify=y_train['status'])\n",
    "\n",
    "    # initialize scaling\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # fit model\n",
    "    fit = scaler.fit(X_train[numericColumns])\n",
    "\n",
    "    # transform\n",
    "    X_train[numericColumns] = fit.transform(X_train[numericColumns])\n",
    "    X_val[numericColumns] = fit.transform(X_val[numericColumns])\n",
    "    X_test[numericColumns] = fit.transform(X_test[numericColumns])\n",
    "    \n",
    "    # display shape\n",
    "    print(f\"Training Dependent Shape: {X_train.shape} & Label Shape: {y_train.shape}\")\n",
    "    print(f\"Validation Dependent Shape: {X_val.shape} & Label Shape: {y_val.shape}\")\n",
    "    print(f\"Testing Dependent Shape: {X_test.shape} & Label Shape: {y_test.shape}\")\n",
    "\n",
    "    return  X, y, X_train, X_test, X_val, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1930dab-39e8-4f1a-aa01-3491c71d9503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dependent Shape: (9507, 204) & Label Shape: (9507,)\n",
      "Validation Dependent Shape: (2377, 204) & Label Shape: (2377,)\n",
      "Testing Dependent Shape: (2972, 204) & Label Shape: (2972,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "X, y, X_train, X_test, X_val, y_train, y_val, y_test = EncodeDummyScaleTrainValTest(X, y, nominalCol, numericCol, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ace58-b643-4961-91ed-2c2dbadc4067",
   "metadata": {},
   "source": [
    "#### Deep Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07f43bb0-1ad1-4717-ac2e-bc14441b74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X.values.astype(float))\n",
    "        self.T = torch.FloatTensor(y['time'].copy())\n",
    "        self.E = torch.FloatTensor(y['status'].copy())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.T[idx], self.E[idx]\n",
    "\n",
    "\n",
    "class DeepSurv(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(DeepSurv, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.BatchNorm1d(dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def negative_log_likelihood(risk_pred, T, E):\n",
    "    hazard_ratio = torch.exp(risk_pred)\n",
    "    log_risk = torch.log(torch.cumsum(hazard_ratio, dim=0) + 1e-5)\n",
    "    uncensored_likelihood = risk_pred.T - log_risk\n",
    "    censored_likelihood = uncensored_likelihood * E\n",
    "    num_observed_events = torch.sum(E)\n",
    "    return -torch.sum(censored_likelihood) / num_observed_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1adc709f-8d08-41b4-a76d-a48d23be95b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: -1276.4071\n",
      "Epoch 2/25, Loss: -2003.3231\n",
      "Epoch 3/25, Loss: -2650.6519\n",
      "Epoch 4/25, Loss: -3030.2876\n",
      "Epoch 5/25, Loss: -3408.9907\n",
      "Epoch 6/25, Loss: -3524.3610\n",
      "Epoch 7/25, Loss: -3647.7494\n",
      "Epoch 8/25, Loss: -4028.2720\n",
      "Epoch 9/25, Loss: -3930.9600\n",
      "Epoch 10/25, Loss: -4311.4199\n",
      "Epoch 11/25, Loss: -4662.9461\n",
      "Epoch 12/25, Loss: -4285.2670\n",
      "Epoch 13/25, Loss: -4967.6029\n",
      "Epoch 14/25, Loss: -4624.8282\n",
      "Epoch 15/25, Loss: -5152.2506\n",
      "Epoch 16/25, Loss: -5370.1319\n",
      "Epoch 17/25, Loss: -5543.0049\n",
      "Epoch 18/25, Loss: -5367.8134\n",
      "Epoch 19/25, Loss: -5643.1104\n",
      "Epoch 20/25, Loss: -5786.1860\n",
      "Epoch 21/25, Loss: -5755.7036\n",
      "Epoch 22/25, Loss: -5521.3559\n",
      "Epoch 23/25, Loss: -6163.2344\n",
      "Epoch 24/25, Loss: -5955.1717\n",
      "Epoch 25/25, Loss: -7070.2178\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 204  # Adjust based on your dataset\n",
    "hidden_dims = [128, 64, 32]\n",
    "learning_rate = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 25\n",
    "\n",
    "\n",
    "# Assuming X, T, and E are your features, event times, and event \n",
    "dataset = SurvivalDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DeepSurv(input_dim, hidden_dims)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_T, batch_E in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        risk_pred = model(batch_X)\n",
    "        loss = -negative_log_likelihood(risk_pred, batch_T, batch_E)  # Negative because we want to maximize likelihood\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    scheduler.step(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# After training, you can use model.eval() and then model(X) to get risk predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa8d24-1e72-4029-a940-179a1631aad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
